{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PubMed():\n",
      "==================\n",
      "Number of graphs: 1\n",
      "Number of features: 500\n",
      "Number of classes: 3\n",
      "\n",
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures\n",
    "\n",
    "dataset = Planetoid(root='../datasets/Planetoid', name='PubMed',\n",
    "                    transform=NormalizeFeatures())\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}:')\n",
    "print('==================')\n",
    "print(f'Number of graphs: {len(dataset)}')\n",
    "print(f'Number of features: {dataset.num_features}')\n",
    "print(f'Number of classes: {dataset.num_classes}')\n",
    "\n",
    "data = dataset[0]  # Get the first graph object.\n",
    "\n",
    "print()\n",
    "print(data)\n",
    "\n",
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "cluster_data = ClusterData(data, num_parts=128)  # 1. Create subgraphs.\n",
    "train_loader = ClusterLoader(cluster_data, batch_size=32,\n",
    "                             shuffle=True)  # 2. Stochastic partioning scheme."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train(model):\n",
    "    model.train()\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=0.01,\n",
    "                                 weight_decay=5e-4)\n",
    "    for sub_data in train_loader:  # Iterate over each mini-batch.\n",
    "        optimizer.zero_grad()  # Clear gradients.\n",
    "        out = model(sub_data.x,\n",
    "                    sub_data.edge_index)  # Perform a single forward pass.\n",
    "        loss = criterion(\n",
    "            out[sub_data.train_mask], sub_data.y[sub_data.train_mask]\n",
    "        )  # Compute the loss solely based on the training nodes.\n",
    "        loss.backward()  # Derive gradients.\n",
    "        optimizer.step()  # Update parameters based on gradients.\n",
    "\n",
    "\n",
    "def test(model):\n",
    "    model.eval()\n",
    "    out = model(data.x, data.edge_index)\n",
    "    pred = out.argmax(dim=1)  # Use the class with highest probability.\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask] == data.y[\n",
    "            mask]  # Check against ground-truth labels.\n",
    "        accs.append(int(correct.sum()) /\n",
    "                    int(mask.sum()))  # Derive ratio of correct predictions.\n",
    "    return accs\n",
    "\n",
    "\n",
    "def run(model, epochs=5):\n",
    "    for epoch in range(epochs):\n",
    "        loss = train(model)\n",
    "        train_acc, val_acc, test_acc = test(model)\n",
    "        print(\n",
    "            f'Epoch: {epoch:03d}, Train: {train_acc:.4f}, Val Acc: {val_acc:.4f}, Test Acc: {test_acc:.4f}'\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import copy\n",
    "\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import (\n",
    "    Aggregation,\n",
    "    MaxAggregation,\n",
    "    MeanAggregation,\n",
    "    MultiAggregation,\n",
    "    SAGEConv,\n",
    "    SoftmaxAggregation,\n",
    "    StdAggregation,\n",
    "    SumAggregation,\n",
    "    VarAggregation,\n",
    ")\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels, aggr='mean', aggr_kwargs=None):\n",
    "        super().__init__()\n",
    "        self.conv1 = SAGEConv(\n",
    "            dataset.num_node_features,\n",
    "            hidden_channels,\n",
    "            aggr=aggr,\n",
    "            aggr_kwargs=aggr_kwargs,\n",
    "        )\n",
    "        self.conv2 = SAGEConv(\n",
    "            hidden_channels,\n",
    "            dataset.num_classes,\n",
    "            aggr=copy.deepcopy(aggr),\n",
    "            aggr_kwargs=aggr_kwargs,\n",
    "        )\n",
    "\n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=mean)\n",
      "  (conv2): SAGEConv(16, 3, aggr=mean)\n",
      ")\n",
      "Epoch: 000, Train: 0.3500, Val Acc: 0.2060, Test Acc: 0.4000\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.4180, Test Acc: 0.4080\n",
      "Epoch: 002, Train: 0.6500, Val Acc: 0.5660, Test Acc: 0.4500\n",
      "Epoch: 003, Train: 0.3500, Val Acc: 0.4220, Test Acc: 0.4370\n",
      "Epoch: 004, Train: 0.7000, Val Acc: 0.4580, Test Acc: 0.7180\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr='mean')\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=MeanAggregation())\n",
      "  (conv2): SAGEConv(16, 3, aggr=MeanAggregation())\n",
      ")\n",
      "Epoch: 000, Train: 0.3500, Val Acc: 0.1960, Test Acc: 0.3010\n",
      "Epoch: 001, Train: 0.3833, Val Acc: 0.4280, Test Acc: 0.4080\n",
      "Epoch: 002, Train: 0.5500, Val Acc: 0.5240, Test Acc: 0.4320\n",
      "Epoch: 003, Train: 0.4000, Val Acc: 0.4300, Test Acc: 0.4990\n",
      "Epoch: 004, Train: 0.7500, Val Acc: 0.4860, Test Acc: 0.7180\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr=MeanAggregation())\n",
    "print(model)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### REMARK: It's not learning when I use max aggregation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=max)\n",
      "  (conv2): SAGEConv(16, 3, aggr=max)\n",
      ")\n",
      "Epoch: 000, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 002, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 003, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 004, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr='max')\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=MaxAggregation())\n",
      "  (conv2): SAGEConv(16, 3, aggr=MaxAggregation())\n",
      ")\n",
      "Epoch: 000, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 002, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 003, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 004, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr=MaxAggregation())\n",
    "print(model)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### list of aggregations: same problem with max"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=['mean', 'max', 'sum', 'std', 'var'])\n",
      "  (conv2): SAGEConv(16, 3, aggr=['mean', 'max', 'sum', 'std', 'var'])\n",
      ")\n",
      "Epoch: 000, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 002, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 003, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 004, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr=['mean', 'max', 'sum', 'std', 'var'])\n",
    "print(model)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=['mean', 'sum', 'std', 'var'])\n",
      "  (conv2): SAGEConv(16, 3, aggr=['mean', 'sum', 'std', 'var'])\n",
      ")\n",
      "Epoch: 000, Train: 0.3500, Val Acc: 0.2480, Test Acc: 0.3830\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.2080, Test Acc: 0.4470\n",
      "Epoch: 002, Train: 0.6333, Val Acc: 0.5880, Test Acc: 0.6240\n",
      "Epoch: 003, Train: 0.4167, Val Acc: 0.4640, Test Acc: 0.6580\n",
      "Epoch: 004, Train: 0.5167, Val Acc: 0.5740, Test Acc: 0.6850\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr=['mean', 'sum', 'std', 'var'])\n",
    "print(model)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=['MeanAggregation()', 'SumAggregation()', 'StdAggregation()', 'VarAggregation()'])\n",
      "  (conv2): SAGEConv(16, 3, aggr=['MeanAggregation()', 'SumAggregation()', 'StdAggregation()', 'VarAggregation()'])\n",
      ")\n",
      "Epoch: 000, Train: 0.3667, Val Acc: 0.2940, Test Acc: 0.4840\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.2740, Test Acc: 0.5750\n",
      "Epoch: 002, Train: 0.5500, Val Acc: 0.4460, Test Acc: 0.6050\n",
      "Epoch: 003, Train: 0.4167, Val Acc: 0.3620, Test Acc: 0.6230\n",
      "Epoch: 004, Train: 0.5333, Val Acc: 0.5320, Test Acc: 0.6410\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(\n",
    "    16, aggr=[\n",
    "        MeanAggregation(),\n",
    "        # MaxAggregation(),\n",
    "        SumAggregation(),\n",
    "        StdAggregation(),\n",
    "        VarAggregation(),\n",
    "    ])\n",
    "print(model)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=['MeanAggregation()', 'MaxAggregation()', 'SumAggregation()', 'StdAggregation()', 'VarAggregation()'])\n",
      "  (conv2): SAGEConv(16, 3, aggr=['MeanAggregation()', 'MaxAggregation()', 'SumAggregation()', 'StdAggregation()', 'VarAggregation()'])\n",
      ")\n",
      "Epoch: 000, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 002, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 003, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 004, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(\n",
    "    16, aggr=[\n",
    "        MeanAggregation(),\n",
    "        MaxAggregation(),\n",
    "        SumAggregation(),\n",
    "        StdAggregation(),\n",
    "        VarAggregation(),\n",
    "    ])\n",
    "print(model)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Use a list of mixed modules and strings for aggregations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=['mean', 'MaxAggregation()', 'sum', 'StdAggregation()', 'var'])\n",
      "  (conv2): SAGEConv(16, 3, aggr=['mean', 'MaxAggregation()', 'sum', 'StdAggregation()', 'var'])\n",
      ")\n",
      "Epoch: 000, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 002, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 003, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 004, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr=[\n",
    "    'mean',\n",
    "    MaxAggregation(),\n",
    "    'sum',\n",
    "    StdAggregation(),\n",
    "    'var',\n",
    "])\n",
    "print(model)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=['mean', 'sum', 'StdAggregation()', 'var'])\n",
      "  (conv2): SAGEConv(16, 3, aggr=['mean', 'sum', 'StdAggregation()', 'var'])\n",
      ")\n",
      "Epoch: 000, Train: 0.3333, Val Acc: 0.2080, Test Acc: 0.3340\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.2200, Test Acc: 0.5010\n",
      "Epoch: 002, Train: 0.5000, Val Acc: 0.3580, Test Acc: 0.6720\n",
      "Epoch: 003, Train: 0.4333, Val Acc: 0.4500, Test Acc: 0.6340\n",
      "Epoch: 004, Train: 0.3167, Val Acc: 0.4840, Test Acc: 0.6210\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "model = GNN(16, aggr=[\n",
    "    'mean',\n",
    "    # MaxAggregation(),\n",
    "    'sum',\n",
    "    StdAggregation(),\n",
    "    'var',\n",
    "])\n",
    "print(model)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GNN(\n",
      "  (conv1): SAGEConv(500, 16, aggr=MultiAggregation([\n",
      "    SoftmaxAggregation(learn=True),\n",
      "    SoftmaxAggregation(learn=True),\n",
      "    SoftmaxAggregation(learn=True),\n",
      "  ], mode=cat))\n",
      "  (conv2): SAGEConv(16, 3, aggr=MultiAggregation([\n",
      "    SoftmaxAggregation(learn=True),\n",
      "    SoftmaxAggregation(learn=True),\n",
      "    SoftmaxAggregation(learn=True),\n",
      "  ], mode=cat))\n",
      ")\n",
      "Epoch: 000, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 001, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 002, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 003, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n",
      "Epoch: 004, Train: 0.3333, Val Acc: 0.1960, Test Acc: 0.1800\n"
     ]
    }
   ],
   "source": [
    "torch.manual_seed(seed)\n",
    "aggr = MultiAggregation([\n",
    "    SoftmaxAggregation(t=0.01, learn=True),\n",
    "    SoftmaxAggregation(t=1, learn=True),\n",
    "    SoftmaxAggregation(t=100, learn=True),\n",
    "])\n",
    "model = GNN(16, aggr=aggr)\n",
    "print(model)\n",
    "run(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_mol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
