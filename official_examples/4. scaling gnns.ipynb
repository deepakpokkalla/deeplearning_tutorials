{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Scaling GNNs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os \n",
    "import torch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- pyg-lib: includes low-level utilies & optimized operations to support GNNs - faster message passing/graph pooling implementations, memory management"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Dataset: PubMed()\n",
      "# graphs: 1\n",
      "# features: 500\n",
      "# classes: 3\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.datasets import Planetoid\n",
    "from torch_geometric.transforms import NormalizeFeatures # row-normalizes the attributes\n",
    "\n",
    "dataset = Planetoid(root='../datasets/Planetoid', name='PubMed', transform=NormalizeFeatures())\n",
    "\n",
    "print()\n",
    "print(f'Dataset: {dataset}')\n",
    "print(f'# graphs: {len(dataset)}')\n",
    "print(f'# features: {dataset.num_features}')\n",
    "print(f'# classes: {dataset.num_classes}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n",
      "=======\n",
      "# nodes: 19717\n",
      "# edges: 88648\n",
      "avf node degree: 4.50\n",
      "# training nodes: 60\n",
      "train node label rate: 0.003\n",
      "has isolated nodes: False\n",
      "is undirected: True\n",
      "has self loops: False\n"
     ]
    }
   ],
   "source": [
    "data = dataset[0]\n",
    "print()\n",
    "print(data)\n",
    "print(\"=======\")\n",
    "\n",
    "# gather some info \n",
    "print(f'# nodes: {data.num_nodes}')\n",
    "print(f'# edges: {data.num_edges}')\n",
    "print(f'avf node degree: {data.num_edges/data.num_nodes:.2f}')\n",
    "print(f'# training nodes: {data.train_mask.sum()}')\n",
    "print(f'train node label rate: {int(data.train_mask.sum())/data.num_nodes:.3f}')\n",
    "print(f'has isolated nodes: {data.has_isolated_nodes()}')\n",
    "print(f'is undirected: {data.is_undirected()}')\n",
    "print(f'has self loops: {data.has_self_loops()}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Look into this Cluster-GCN architecture (https://arxiv.org/abs/1905.07953)\n",
    "\n",
    "- ClusterData: converts the \"Data\" object into a dataset of subgraphs containing \"num_parts\" partitions. Clusters/partitions a graph data object into multiple subgraphs. \n",
    "\n",
    "- ClusterLoader: given a \"batch_size\", implements the stochastic partitioning scheme in order to create mini-batches. merges partioned subgraphs and their between-cluster links from a large-scale graph data object to form a mini-batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original graph: Data(x=[19717, 500], edge_index=[2, 88648], y=[19717], train_mask=[19717], val_mask=[19717], test_mask=[19717])\n",
      "\n",
      "# nodes in current batch: 4916 and step: 1\n",
      "Data(x=[4916, 500], y=[4916], train_mask=[4916], val_mask=[4916], test_mask=[4916], edge_index=[2, 16180])\n",
      "# nodes in current batch: 4909 and step: 2\n",
      "Data(x=[4909, 500], y=[4909], train_mask=[4909], val_mask=[4909], test_mask=[4909], edge_index=[2, 15912])\n",
      "# nodes in current batch: 4941 and step: 3\n",
      "Data(x=[4941, 500], y=[4941], train_mask=[4941], val_mask=[4941], test_mask=[4941], edge_index=[2, 15958])\n",
      "# nodes in current batch: 4951 and step: 4\n",
      "Data(x=[4951, 500], y=[4951], train_mask=[4951], val_mask=[4951], test_mask=[4951], edge_index=[2, 18662])\n",
      "\n",
      "Iterated over 19717 of 19717 nodes!\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing METIS partitioning...\n",
      "Done!\n"
     ]
    }
   ],
   "source": [
    "from torch_geometric.loader import ClusterData, ClusterLoader\n",
    "\n",
    "torch.manual_seed(12345)\n",
    "print(f'Original graph: {data}')\n",
    "cluster_data = ClusterData(data,num_parts=128)\n",
    "train_loader = ClusterLoader(cluster_data, batch_size=32, shuffle=True)\n",
    "\n",
    "print()\n",
    "total_num_nodes= 0 \n",
    "for step, sub_data in enumerate(train_loader):\n",
    "    print(f'# nodes in current batch: {sub_data.num_nodes} and step: {step+1}')\n",
    "    print(sub_data)\n",
    "    total_num_nodes += sub_data.num_nodes\n",
    "\n",
    "print()\n",
    "print(f'Iterated over {total_num_nodes} of {data.num_nodes} nodes!')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### TO DO: Since the partitioning is based on METIS algorithm, analyse how METIS devides the graph in each epoch for an FEA mesh. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "GCN(\n",
      "  (conv1): GCNConv(500, 16)\n",
      "  (conv2): GCNConv(16, 3)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import GCNConv\n",
    "\n",
    "class GCN(torch.nn.Module):\n",
    "    def __init__(self, hidden_channels):\n",
    "        super().__init__()\n",
    "        torch.manual_seed(12345)\n",
    "        self.conv1 = GCNConv(dataset.num_features, hidden_channels)\n",
    "        self.conv2 = GCNConv(hidden_channels, dataset.num_classes)\n",
    "    \n",
    "    def forward(self, x, edge_index):\n",
    "        x = self.conv1(x, edge_index)\n",
    "        x = x.relu()\n",
    "        x = F.dropout(x, p=0.5, training=self.training)\n",
    "        x = self.conv2(x, edge_index)\n",
    "        return x\n",
    "\n",
    "model = GCN(hidden_channels=16)\n",
    "print(model)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 001, train acc: 0.3333, val acc: 0.4160, test acc: 0.4070\n",
      "Epoch: 002, train acc: 0.3333, val acc: 0.4160, test acc: 0.4070\n",
      "Epoch: 003, train acc: 0.3333, val acc: 0.4160, test acc: 0.4070\n",
      "Epoch: 004, train acc: 0.3333, val acc: 0.4160, test acc: 0.4070\n",
      "Epoch: 005, train acc: 0.3333, val acc: 0.4160, test acc: 0.4070\n",
      "Epoch: 006, train acc: 0.3333, val acc: 0.4160, test acc: 0.4100\n",
      "Epoch: 007, train acc: 0.4667, val acc: 0.4460, test acc: 0.4600\n",
      "Epoch: 008, train acc: 0.4833, val acc: 0.4940, test acc: 0.5210\n",
      "Epoch: 009, train acc: 0.6333, val acc: 0.5400, test acc: 0.5560\n",
      "Epoch: 010, train acc: 0.6000, val acc: 0.5040, test acc: 0.5420\n",
      "Epoch: 011, train acc: 0.6167, val acc: 0.5260, test acc: 0.5600\n",
      "Epoch: 012, train acc: 0.8000, val acc: 0.6180, test acc: 0.6400\n",
      "Epoch: 013, train acc: 0.9500, val acc: 0.7020, test acc: 0.7320\n",
      "Epoch: 014, train acc: 0.9333, val acc: 0.7260, test acc: 0.7420\n",
      "Epoch: 015, train acc: 0.9167, val acc: 0.6960, test acc: 0.7080\n",
      "Epoch: 016, train acc: 0.8500, val acc: 0.6320, test acc: 0.6360\n",
      "Epoch: 017, train acc: 0.9167, val acc: 0.6860, test acc: 0.6790\n",
      "Epoch: 018, train acc: 0.9833, val acc: 0.7460, test acc: 0.7550\n",
      "Epoch: 019, train acc: 0.9833, val acc: 0.7620, test acc: 0.7490\n",
      "Epoch: 020, train acc: 0.9833, val acc: 0.7600, test acc: 0.7530\n",
      "Epoch: 021, train acc: 0.9833, val acc: 0.7460, test acc: 0.7450\n",
      "Epoch: 022, train acc: 0.9667, val acc: 0.7480, test acc: 0.7530\n",
      "Epoch: 023, train acc: 0.9833, val acc: 0.7460, test acc: 0.7640\n",
      "Epoch: 024, train acc: 0.9833, val acc: 0.7680, test acc: 0.7800\n",
      "Epoch: 025, train acc: 0.9500, val acc: 0.7700, test acc: 0.7820\n",
      "Epoch: 026, train acc: 0.9833, val acc: 0.7780, test acc: 0.7810\n",
      "Epoch: 027, train acc: 0.9833, val acc: 0.7720, test acc: 0.7850\n",
      "Epoch: 028, train acc: 0.9833, val acc: 0.7580, test acc: 0.7860\n",
      "Epoch: 029, train acc: 0.9833, val acc: 0.7600, test acc: 0.7690\n",
      "Epoch: 030, train acc: 0.9833, val acc: 0.7660, test acc: 0.7640\n",
      "Epoch: 031, train acc: 0.9833, val acc: 0.7640, test acc: 0.7520\n",
      "Epoch: 032, train acc: 0.9833, val acc: 0.7820, test acc: 0.7690\n",
      "Epoch: 033, train acc: 0.9833, val acc: 0.7820, test acc: 0.7680\n",
      "Epoch: 034, train acc: 0.9833, val acc: 0.7920, test acc: 0.7770\n",
      "Epoch: 035, train acc: 0.9833, val acc: 0.7600, test acc: 0.7830\n",
      "Epoch: 036, train acc: 0.9833, val acc: 0.7660, test acc: 0.7720\n",
      "Epoch: 037, train acc: 0.9833, val acc: 0.7760, test acc: 0.7770\n",
      "Epoch: 038, train acc: 0.9833, val acc: 0.7840, test acc: 0.7630\n",
      "Epoch: 039, train acc: 0.9833, val acc: 0.7840, test acc: 0.7720\n",
      "Epoch: 040, train acc: 0.9833, val acc: 0.7820, test acc: 0.7830\n",
      "Epoch: 041, train acc: 0.9833, val acc: 0.7600, test acc: 0.7720\n",
      "Epoch: 042, train acc: 0.9833, val acc: 0.7560, test acc: 0.7790\n",
      "Epoch: 043, train acc: 0.9833, val acc: 0.7780, test acc: 0.7800\n",
      "Epoch: 044, train acc: 0.9833, val acc: 0.7840, test acc: 0.7930\n",
      "Epoch: 045, train acc: 0.9833, val acc: 0.7720, test acc: 0.7860\n",
      "Epoch: 046, train acc: 0.9833, val acc: 0.7660, test acc: 0.7850\n",
      "Epoch: 047, train acc: 0.9833, val acc: 0.7720, test acc: 0.7760\n",
      "Epoch: 048, train acc: 0.9833, val acc: 0.7880, test acc: 0.7880\n",
      "Epoch: 049, train acc: 0.9833, val acc: 0.7960, test acc: 0.7850\n",
      "Epoch: 050, train acc: 0.9833, val acc: 0.7960, test acc: 0.7890\n",
      "Max Train Acc: 0.9833, Max Val Acc: 0.7960, Max Test Acc: 0.7930\n"
     ]
    }
   ],
   "source": [
    "model = GCN(hidden_channels=16)\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01, weight_decay=5e-4)\n",
    "criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "\n",
    "    for sub_data in train_loader:\n",
    "        out = model(sub_data.x, sub_data.edge_index)\n",
    "        loss = criterion(out[sub_data.train_mask], sub_data.y[sub_data.train_mask])\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        optimizer.zero_grad()\n",
    "\n",
    "def test():\n",
    "    model.eval()\n",
    "    out = model(data.x,data.edge_index)\n",
    "    pred = out.argmax(dim=1)\n",
    "\n",
    "    accs = []\n",
    "    for mask in [data.train_mask, data.val_mask, data.test_mask]:\n",
    "        correct = pred[mask] == data.y[mask]\n",
    "        accs.append(int(correct.sum())/ int(mask.sum()))\n",
    "    return accs\n",
    "\n",
    "max_train_acc = 0.0\n",
    "max_val_acc = 0.0\n",
    "max_test_acc = 0.0\n",
    "for epoch in range(1,51):\n",
    "    loss = train()\n",
    "    train_acc, val_acc, test_acc = test()\n",
    "    max_train_acc = max(max_train_acc, train_acc)\n",
    "    max_val_acc = max(max_val_acc, val_acc)\n",
    "    max_test_acc = max(max_test_acc, test_acc)\n",
    "    print(f'Epoch: {epoch:03d}, train acc: {train_acc:.4f}, val acc: {val_acc:.4f}, test acc: {test_acc:.4f}')\n",
    "print(f'Max Train Acc: {max_train_acc:.4f}, Max Val Acc: {max_val_acc:.4f}, Max Test Acc: {max_test_acc:.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # # check how to use NormalizeFeatures class\n",
    "\n",
    "# from torch_geometric.data import Data\n",
    "\n",
    "# norm = NormalizeFeatures() # row-normalizes the attributes\n",
    "# temp = torch.randint(1,10,size=(3,4), dtype=float)\n",
    "\n",
    "# temp_data = Data(x=temp)\n",
    "# norm_data = norm(temp_data)\n",
    "\n",
    "# print(temp)\n",
    "# print(norm_data.x)\n",
    "# print(norm_data.x[0,:].sum())\n",
    "# print(norm_data.x[1,:].sum())\n",
    "# print(norm_data.x[2,:].sum())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "gnn_mol",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
